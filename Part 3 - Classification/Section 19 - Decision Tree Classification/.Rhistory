library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
install.packages("ggplot2")
library(readr)
query_hive_756884 <- read_csv("~/Downloads/query_hive_756884.csv")
View(query_hive_756884)
library(readr)
query_hive_756884 <- read_csv("~/Downloads/query_hive_756884.csv")
View(query_hive_756884)
set.seed(7)
km1 = kmeans(dat, 2, nstart=100)
set.seed(7)
km1 = kmeans(query_hive_756884, 2, nstart=100)
plot(dat, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1) , main="K-Means", pch=20, cex=2)
warnings()
plot(query_hive_756884[0], col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1
km1.size()
size km1
size(km1)
library(readr)
Wholesale_customers_data <- read_csv("~/Downloads/Wholesale_customers_data.csv")
View(Wholesale_customers_data)
summary(Wholesale_customers_data)
Summary(query_hive_756884)
summary(query_hive_756884)
top.n.custs <- function (data,cols,n=5) {
}
top.n.custs <- function (data,cols,n=5) { #Requires some data frame and the top N to remove
idx.to.remove <-integer(0) #Initialize a vector to hold customers being removed
for (c in cols){ # For every column in the data we passed to this function
col.order <-order(data[,c],decreasing=T) #Sort column "c" in descending order (bigger on top)
#Order returns the sorted index (e.g. row 15, 3, 7, 1, ...) rather than the actual values sorted.
idx <-head(col.order, n) #Take the first n of the sorted column C to
idx.to.remove <-union(idx.to.remove,idx) #Combine and de-duplicate the row ids that need to be removed
}
return(idx.to.remove) #Return the indexes of customers to be removed
}
top.custs <-top.n.custs(data,cols=3:8,n=5)
top.n.custs <- function (data,cols,n=5) { #Requires some data frame and the top N to remove
idx.to.remove <-integer(0) #Initialize a vector to hold customers being removed
for (c in cols){ # For every column in the data we passed to this function
col.order <-order(data[,c],decreasing=T) #Sort column "c" in descending order (bigger on top)
#Order returns the sorted index (e.g. row 15, 3, 7, 1, ...) rather than the actual values sorted.
idx <-head(col.order, n) #Take the first n of the sorted column C to
idx.to.remove <-union(idx.to.remove,idx) #Combine and de-duplicate the row ids that need to be removed
}
return(idx.to.remove) #Return the indexes of customers to be removed
}
top.custs <-top.n.custs(data,cols=3:8,n=5)
length(top.custs) #How Many Customers to be Removed?
data[top.custs,] #Examine the customers
data.rm.top<-data[-c(top.custs),] #Remove the Customers
top.n.custs <- function (data,cols,n=5) {
idx.to.remove <-integer(0)
for (c in cols){
col.order <-order(data[,c],decreasing=T)
idx <-head(col.order, n)
idx.to.remove <-union(idx.to.remove,idx)
return(idx.to.remove)
}
top.custs <-top.n.custs(data,cols=3:8,n=5)
length(top.custs)
data[top.custs,]
data.rm.top<-data[-c(top.custs),]
}
plot(query_hive_756884[0], col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1 = kmeans(query_hive_756884, 20, nstart=100)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1$cluster ) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1$cluster ) , main="K-Means result with 2 clusters", pch=20, cex=3)
plot(query_hive_756884, col =(km1$cluster -1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
table(km1)
summary(km1)
km1 = kmeans(query_hive_756884, centers=1, iter.max = 10, nstart=1)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1 = kmeans(query_hive_756884, centers=10, iter.max = 10, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1 = kmeans(query_hive_756884, centers=3, iter.max = 10, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
library(readr)
query_hive_756884 <- read_csv("~/Downloads/query_hive_756884.csv",
col_types = cols(`events_flat_web_20170916_susp_summary_user_id_hotel.action/user_id` = col_skip()))
View(query_hive_756884)
km1 = kmeans(query_hive_756884, centers=3, iter.max = 10, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
library(readr)
query_hive_756884 <- read_csv("~/Downloads/query_hive_756884.csv",
col_types = cols(`action/user_id` = col_skip()))
View(query_hive_756884)
km1 = kmeans(query_hive_756884, centers=3, iter.max = 10, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1$cluster +1
km1$cluster
km1
km1 = kmeans(query_hive_756884, centers=5, iter.max = 100, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1
km1 = kmeans(query_hive_756884, centers=2, iter.max = 100, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km1 = kmeans(query_hive_756884, centers=5, iter.max = 100, nstart=50)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
library(readr)
query_hive_756884 <- read_csv("~/Downloads/query_hive_756884.csv",
col_types = cols(`action/user_id` = col_skip(),
name_types = col_skip(), num_of_res = col_skip(),
persona_types = col_skip()))
View(query_hive_756884)
plot(query_hive_756884, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
library(readr)
query_hive_760807 <- read_csv("~/Downloads/query_hive_760807.csv",
col_types = cols(`action/user_id` = col_skip(),
num_of_res = col_skip()))
View(query_hive_760807)
km807 = kmeans(query_hive_760807, centers=5, iter.max = 100, nstart=50)
plot(query_hive_756884, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
km807 = kmeans(query_hive_760807, centers=10, iter.max = 100, nstart=50)
plot(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
points(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
hist(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
plot(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=1, cex=0.5)
plot(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=21, cex=1.0)
plot(query_hive_760807, col =(km807$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=1.5)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=1, cex=0.5)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=20, cex=1.5)
km807 = kmeans(query_hive_760807, centers=10, iter.max = 1000, nstart=50)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=20, cex=1.5)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=20, cex=3.5)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=20, cex=2.0)
km807 = kmeans(query_hive_760807, centers=3, iter.max = 1000, nstart=50)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=20, cex=1.5)
km807 = kmeans(query_hive_760807, centers=4, iter.max = 10000, nstart=2)
plot(query_hive_760807, col =(km807$cluster +1) , main="UserID", pch=20, cex=1.5)
install.packages(c("backports", "boot", "Matrix", "mgcv", "Rcpp", "rJava"))
library("KernSmooth", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
setwd("~/dev/Machine Learning A-Z/Part 3 - Classification/Section 18 - Naive Bayes")
# Naive Bayes
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
View(dataset)
dataset
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting SVM to the Training set
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)
classifier
y_pred = predict(classifier, newdata = test_set[-3])
y_pred
cm = table(test_set[, 3], y_pred)
cm
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/dev/Machine Learning A-Z/Part 3 - Classification/Section 19 - Decision Tree Classification")
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Decision Tree Classification to the Training set
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
y_pred
cm
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Decision Tree Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree Classification (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Plotting the tree
plot(classifier)
text(classifier)
y_pred
test_set
y_pred
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Decision Tree Classification to the Training set
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Decision Tree Classification to the Training set
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
View(training_set)
View(training_set)
View(test_set)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
View(test_set)
View(test_set)
View(grid_set)
View(set)
View(training_set)
y_grid
y_pred
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
y_pred
summary(y_pred)
test_set
test_set[1]
test_set[0]
test_set[1]
test_set
test_set[0]
test_set[1]
test_set[2]
test_set[3]
test_set[-1]
test_set[--3]
test_set[-3]
test_set[-3,-2]
test_set[-3,-2,-1]
test_set[,-1]
test_set[,-1,]
test_set[]
# Decision Tree Classification
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Decision Tree Classification to the Training set
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Decision Tree Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree Classification (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Plotting the tree
plot(classifier)
text(classifier)
# Decision Tree Classification
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Decision Tree Classification to the Training set
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Decision Tree Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree Classification (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Plotting the tree
plot(classifier)
text(classifier)
